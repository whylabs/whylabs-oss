from dataclasses import dataclass
from functools import partial
from logging import getLogger
from typing import List, Optional, Tuple, Type, Union

import nltk
import numpy as np
import numpy.typing as npt
import pandas as pd
import torch
from nltk.downloader import Downloader
from nltk.tokenize import sent_tokenize  # pyright: ignore[reportUnknownVariableType]
from openai.types.chat import ChatCompletionMessageParam
from sentence_transformers import util
from typing_extensions import Unpack

from langkit.core.metric import Metric, SingleMetric, SingleMetricResult
from langkit.openai.hallucination_types import LLMInvoker, OpenAIChoiceArg
from langkit.openai.openai import LLMInvocationParams, get_llm
from langkit.transformer import EmbeddingChoiceArg, EmbeddingOptions
from whylabs_llm_toolkit.data.scripts.targets import SentenceTransformerEncoder
from whylabs_llm_toolkit.settings import get_settings

_diagnostic_logger = getLogger(__name__)


_llm_model_system_message = "The following is a conversation with an AI assistant."


@dataclass(frozen=True)
class ChatLog:
    prompt: str
    response: str
    errors: Optional[str] = None
    messages: Optional[List[ChatCompletionMessageParam]] = None
    total_tokens: Optional[int] = None

    def to_dict(self):
        return {
            "prompt": self.prompt,
            "response": self.response,
            "errors": self.errors,
            "total_tokens": self.total_tokens,
        }


@dataclass(frozen=True)
class Conversation:
    invoker: LLMInvoker

    def send_prompt(self, prompt: str, **params: Unpack[LLMInvocationParams]) -> ChatLog:
        messages: List[ChatCompletionMessageParam] = [
            {
                "role": "system",
                "content": _llm_model_system_message,
            },
            {
                "role": "user",
                "content": prompt,
            },
        ]

        try:
            # need to support Completion
            response = self.invoker.completion(messages=messages, **params)
        except Exception as e:
            return ChatLog(prompt, "", f"{e}")

        result: str = ""
        for choice in response.choices:
            result += choice.message.content or ""
            messages.append({"role": "assistant", "content": choice.message.content or ""})

        return ChatLog(
            prompt,
            result,
            messages=messages,
            total_tokens=response.usage.total_tokens if response.usage else None,
        )


@dataclass(frozen=True)
class ConsistencyResult:
    """
    This class represents the result of a consistency check.

    llm_score: float
        The score given by the llm-based check. It is the average of the scores
        given by the llm between the response and each sample.
    semantic_score: float
        The score given by the semantic similarity-based check.
    final_score: float
        The average of the llm_score and the semantic_score.
    total_tokens: int
        The total number of tokens used by the llm to generate the samples and
        to check the consistency.
    samples: List[str]
        The list of samples generated by the llm.
    response: str
        The response to be checked.


    """

    llm_score: float
    semantic_score: float
    final_score: float
    total_tokens: int
    samples: List[str]
    response: str


class ConsistencyChecker:
    """
    This class is responsible for checking the consistency of a response
    generated by a language model.

    It uses a sample generator language model to generate samples of responses
    to a prompt.The llm used to generate the samples should be the same as the one used
    to generate the response.


    The additional samples are used to check the consistence between samples and
    original response. The consistency check is done by combining an llm-based check
    with a semantic similarity-based check.

    This approach was inspired by: https://arxiv.org/abs/2303.08896
    """

    def __init__(self, llm: LLMInvoker, num_samples: int, choice: Union[EmbeddingChoiceArg, SentenceTransformerEncoder]):
        self.num_samples = num_samples
        self.llm = llm
        # self.sample_generator_llm = replace(llm, temperature=1)
        # self.consistency_checker_llm = replace(llm, temperature=0)
        try:
            self.embeddings_encoder = choice.get_encoder()  # pyright: ignore[reportAttributeAccessIssue, reportUnknownMemberType]
        except AttributeError:
            self.embeddings_encoder = EmbeddingOptions.get_encoder(choice)  # pyright: ignore[reportArgumentType]

    def convert_score(self, result: str):
        categories = {
            "major inaccurate": 1.0,
            "minor inaccurate": 0.5,
            "accurate": 0.0,
        }
        score = categories.get(result.lower().strip(), None)
        if score is None:
            _diagnostic_logger.error(f"Invalid result from consistency checker: {result}. Valid results are {categories.keys()}")
            score = 0.0
        return score

    def get_samples(self, prompt: str) -> List[ChatLog]:
        samples: List[ChatLog] = []
        for _ in range(self.num_samples):
            response: ChatLog = Conversation(self.llm).send_prompt(prompt, temperature=1)
            if response.errors:
                raise Exception(f"Response Hallucination - Error generating sample: {response.errors}")
            samples.append(response)
        return samples

    def sentence_semantic_score(self, response_sentence: str, samples_list: List[str]) -> float:
        sample_similarities: List[float] = []
        response_embedding: Union[torch.Tensor, npt.NDArray[np.float64]] = self.embeddings_encoder.encode((response_sentence,))  # pyright: ignore[reportUnknownMemberType, reportUnknownMemberType, reportUnknownVariableType]
        for sample in samples_list:
            sample_sentences = sent_tokenize(sample)
            sentence_similarities = [
                util.pytorch_cos_sim(response_embedding, self.embeddings_encoder.encode((sample_sentence,))).item()  # pyright: ignore[reportArgumentType, reportUnknownMemberType]
                for sample_sentence in sample_sentences
            ]
            if sentence_similarities:
                sample_similarities.append(max(sentence_similarities))
        sentence_score = 1 - sum(sample_similarities) / len(sample_similarities)
        return sentence_score

    def semantic_score(self, response: str, additional_samples: List[ChatLog]) -> float:
        """
        This function calculates the semantic score between the response and
        the samples generated by the llm.

        The semantic score is calculated per sentence and then averaged.

        """
        response_sentences: List[str] = sent_tokenize(response)
        samples_list: List[str] = [sample.response for sample in additional_samples]
        response_similarities: List[float] = [self.sentence_semantic_score(sentence, samples_list) for sentence in response_sentences]
        final_score = sum(response_similarities) / len(response_similarities)
        return final_score

    def llm_consistency_check(self, response: str, additional_samples: List[ChatLog]) -> Tuple[float, int]:
        """
        This function calculates the llm score by asking the llm to perform a consistency check between the
        response and each sample.

        The llm score is calculated per half of the response and then averaged.

        """
        llm_scores: List[float] = []
        response_sentences: List[str] = sent_tokenize(response)
        split_at = len(response_sentences) // 2
        response_halves: List[str] = [
            "".join(response_sentences[:split_at]),
            "".join(response_sentences[split_at:]),
        ]
        response_halves = [half for half in response_halves if half]
        total_tokens: int = 0
        for response_half in response_halves:
            llm_scores_half: List[float] = []
            for sample in additional_samples:
                prompt = f"""Context: {sample.response}

                Passage: {response_half}

                Is the passage supported by the context above?
                Answer between: Accurate, Minor Inaccurate, Major Inaccurate

                Don't include additional information/explanation. Please answer only with the options above.

                Answer:
                """
                result: ChatLog = Conversation(self.llm).send_prompt(prompt, temperature=0)
                llm_score = self.convert_score(result.response)
                llm_scores_half.append(llm_score)
                total_tokens += result.total_tokens  # pyright: ignore[reportOperatorIssue,reportUnknownVariableType]
            llm_scores.append(sum(llm_scores_half) / len(llm_scores_half))
        final_score: float = sum(llm_scores) / len(llm_scores)
        return (final_score, total_tokens)  # pyright: ignore[reportUnknownVariableType]

    def consistency_check_with_samples(self, response: str, additional_samples: List[ChatLog]) -> ConsistencyResult:
        # TODO: this appears to be unused. Verify change.
        additional_samples = [ChatLog("", sample.response) for sample in additional_samples]
        llm_score, tokens_usage = self.llm_consistency_check(response, additional_samples)

        semantic_score: float = self.semantic_score(response, additional_samples)
        consistency_result = ConsistencyResult(
            llm_score=llm_score,
            semantic_score=semantic_score,
            final_score=(llm_score + semantic_score) / 2,
            total_tokens=tokens_usage,
            samples=[sample.response for sample in additional_samples],
            response=response,
        )
        return consistency_result

    def consistency_check(self, prompt: str, response: Optional[str] = None) -> ConsistencyResult:
        total_tokens = 0

        if not response:
            result: ChatLog = Conversation(self.llm).send_prompt(prompt)
            if result.total_tokens:
                total_tokens += result.total_tokens
            if result.response:
                response = result.response
            else:
                raise Exception(f"Error generating response: {result.errors}")

        additional_samples: List[ChatLog] = self.get_samples(prompt)
        total_tokens += sum([sample.total_tokens if sample.total_tokens else 0 for sample in additional_samples])

        llm_score, tokens_usage = self.llm_consistency_check(response, additional_samples)
        total_tokens += tokens_usage

        semantic_score = self.semantic_score(response, additional_samples)
        consistency_result = ConsistencyResult(
            llm_score=llm_score,
            semantic_score=semantic_score,
            final_score=(llm_score + semantic_score) / 2,
            total_tokens=total_tokens,
            samples=[sample.response for sample in additional_samples],
            response=response,
        )
        return consistency_result


def hallucination_metric(
    input_column_name: str = "prompt",
    output_column_name: str = "response",
    embedding: Union[EmbeddingChoiceArg, SentenceTransformerEncoder] = "default",
    llm_choice: Optional[OpenAIChoiceArg] = None,
    num_samples: Optional[int] = None,
    openai_model: Optional[str] = None,
    checker_class: Type[ConsistencyChecker] = ConsistencyChecker,
) -> Metric:
    default_num_samples = 2  # A single sample tends to be too noisy

    if openai_model is None:
        llm = get_llm(llm_choice or "default")
    else:
        llm = get_llm(llm_choice or "default", model=openai_model)

    _diagnostic_logger.info(
        "Info: the response_hallucination metric module performs additionall LLM calls to check the consistency of the response."
    )
    checker = checker_class(llm, num_samples or default_num_samples, embedding)

    def init():
        settings = get_settings()

        # Enforce some constraints when making the metric. We know we need these settings if we're using azure.
        if llm_choice == "azure":
            if not settings.AZURE_OPENAI_ENDPOINT:
                raise Exception("AZURE_OPENAI_ENDPOINT env var is not set when trying to use Azure OpenAI API")

            if not settings.AZURE_OPENAI_KEY:
                raise Exception("AZURE_OPENAI_KEY env var is not set when trying to use Azure OpenAI API")
        else:
            if not settings.OPENAI_API_KEY:
                raise Exception("OPENAI_API_KEY env var is not set when trying to use OpenAI API")

    def cache_assets():
        id = "punkt_tab"
        if not Downloader().is_installed(id):  # pyright: ignore[reportUnknownMemberType]
            nltk.download(id, raise_on_error=True)  # pyright: ignore[reportUnknownMemberType]

    def udf(text: pd.DataFrame) -> SingleMetricResult:
        series_result: List[Optional[float]] = []
        for prompt, response in zip(text[input_column_name], text[output_column_name]):  # pyright: ignore[reportUnknownVariableType, reportUnknownArgumentType]
            try:
                result: ConsistencyResult = checker.consistency_check(prompt, response)  # pyright: ignore[reportUnknownArgumentType]
                series_result.append(result.final_score)
            except Exception as e:
                _diagnostic_logger.error(e)
                series_result.append(None)

        return SingleMetricResult(metrics=series_result)

    return SingleMetric(
        name=f"{output_column_name}.hallucination.hallucination_score",
        input_names=[input_column_name, output_column_name],
        evaluate=udf,
        cache_assets=cache_assets,
        init=init,
    )


prompt_response_hallucination_metric = partial(hallucination_metric, "prompt", "response")
